{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym Env\n",
    "\n",
    "> Create a custom GYM environment to simulate trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp betting_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gym\n",
    "import numpy as np\n",
    "import numexpr\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from betting_env.asian_handicap_pnl import *\n",
    "from betting_env.datastructure.odds import MarketOdds\n",
    "from betting_env.config.mongo import mongo_init\n",
    "from pymatchpred.datastructure.lineup import TeamSheet\n",
    "from pymatchpred.datastructure.features import GameFeatures\n",
    "from d3rlpy.preprocessing import ActionScaler, RewardScaler, Scaler\n",
    "from d3rlpy.dataset import TransitionMiniBatch\n",
    "from inspect import signature\n",
    "from typing_extensions import Protocol\n",
    "from infi.traceback import pretty_traceback_and_exit_decorator\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    ClassVar,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Union,\n",
    "    Type,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide here a simple class that stores our RL observations. The first format is a numerical numpy array that holds numerical game information and the second format is a user-friendly output to show game information in each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Observation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_id: int,  # game Id\n",
    "        lineups: np.ndarray,  # lineups\n",
    "        lineups_ids: np.ndarray,  # lineups opta Ids\n",
    "        teams_names: pd.core.series.Series,  # team names\n",
    "        teams_ids: np.ndarray,  # teams opta Ids\n",
    "        betting_market: np.ndarray,  # odds\n",
    "        ah_line: float,  # Asian handicap line\n",
    "    ):\n",
    "        self.game_id = game_id\n",
    "        self.lineups = lineups\n",
    "        self.lineups_ids = lineups_ids\n",
    "        self.teams_names = teams_names\n",
    "        self.teams_ids = teams_ids\n",
    "        self.betting_market = betting_market\n",
    "        self.ah_line = ah_line\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"numerical output\"\"\"\n",
    "        numerical_observation = np.concatenate(\n",
    "            (\n",
    "                np.array([self.game_id]).reshape(1, -1),  # opta gameId\n",
    "                np.array([self.teams_ids]),  # teams Opta Ids\n",
    "                np.array([self.lineups_ids[0]]),  # home lineup (players opta Id)\n",
    "                np.array([self.lineups_ids[1]]),  # away lineup (players opta Id)\n",
    "                self.betting_market,  # odds (1x2 and AH)\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return numerical_observation\n",
    "\n",
    "    def observation_pretty_output(self):\n",
    "        \"\"\"User-friendly output\"\"\"\n",
    "        observation = {\n",
    "            \"gameId\": [self.game_id],\n",
    "            \"homeTeam\": [self.teams_names[0]],\n",
    "            \"awayTeam\": [self.teams_names[1]],\n",
    "            \"homeLineup\": self.lineups[0],\n",
    "            \"awayLineup\": self.lineups[1],\n",
    "            \"odds1\": self.betting_market[:, 0:3][0][0],\n",
    "            \"oddsX\": self.betting_market[:, 0:3][0][1],\n",
    "            \"odds2\": self.betting_market[:, 0:3][0][2],\n",
    "            \"ahLine\": [self.ah_line],\n",
    "            \"oddsAhHome\": self.betting_market[:, 3:][0][0],\n",
    "            \"oddsAhAway\": self.betting_market[:, 3:][0][1],\n",
    "        }\n",
    "\n",
    "        return pd.DataFrame(observation, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betting Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a branch of machine learning (ML) that focuses on the complex and all-encompassing issue of training a system to behave appropriately in a given situation. Only the value of the reward and observations made about the environment are used to drive learning. The generality of this model allows it to be used in a wide range of real-world contexts, from gaming to the improvement of sophisticated industrial procedures.\n",
    "\n",
    "In this perspective, the environment and the agent are two crucial elements of RL. The environment is the Agent's world where it exists and the Agent can engage in interactions with this environment by taking certain actions which cannot change the environment's laws or dynamics.\n",
    "\n",
    "The goal of this work is to develop a Deep Reinforcement Learning environment that simulates a betting strategy. The theory underlying this environment is quite straightforward: placing a bet entails selecting a potential outcome, deciding on a stake size, multiplying it by the winning odds, and then deducting the initial wager and any losses.\n",
    "\n",
    "Here, the agent can choose a discrete action space with the following options for actions: \n",
    "- choose a small, medium, or big wager size. And,\n",
    "- Wagering on the home, draw, or away (1X2 lines), or on the home or away Asian line.\n",
    "\n",
    "It should be noted that the agent can only choose one action from the 15 preceding suggestions.\n",
    "\n",
    "\n",
    "\n",
    "In addition, our RL betting environment is a subclass of an OpenAI Gym environment, with an observation space equal to (gameId, home team lineup, away team lineup, betting line(1X2, Asian handicap) and selected odds) and an action space equal to the options available to the agent (the wager size and the chosen outcome). \n",
    "\n",
    "A simple action in the environment consists of getting the current observation and placing a bet. The reward (the investment return), which can be positive or negative, is then calculated and deducting the total amount of the wager.\n",
    "\n",
    "The line that the agent should select will determine the determined amount. In other words, if we bet 1X2 on the line, we can say that the profit can be expressed as follows:\n",
    "\n",
    "    - profit = (bet * invested_amount * results * odds) - (bet * invested_amount)\n",
    "             = reward - expense\n",
    "\n",
    "    with : \n",
    "    * bet = the chosen outcome or side (Home win, Draw, Away win)\n",
    "    * invested_amount = bet size\n",
    "    * results = postgame outcome\n",
    "    * odds = 1X2 odds\n",
    "\n",
    "\n",
    "If the agent selects the Asian handicap, the profit will depend on the outcome of the game's goal-difference and the chosen line (Half Integer Line, Integer Line, Quarter Integer Line). The agent can win the full bet or just the half of it, lose the full bet or just the half of it, or return its stake.\n",
    "\n",
    "For instance, the profit could be expressed as follows if the agent had won the wager:\n",
    "\n",
    "    - profit = invested_amount * (odds_ah -1)\n",
    "\n",
    "If the agent wins the half of the bet :\n",
    "\n",
    "    - profit = invested_amount * ((odds_ah - 1) * 0.5))\n",
    "\n",
    "    with : \n",
    "    * invested_amount = bet size\n",
    "    * odds_ah = Asian Handicap odds\n",
    "\n",
    "\n",
    "When there are no more games to play or the user's bank balance is exhausted, an episode will be concluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class BettingEnv(gym.Env):\n",
    "    \"\"\"Base class for sports betting environments.\n",
    "\n",
    "    Creates an OpenAI Gym environment that supports betting a (small / medium / large) amount\n",
    "    on a single outcome for a single game.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observation_space : gym.spaces.Box\n",
    "        The observation space for the environment.\n",
    "        The observation space shape is (1, N) where N is the number of possible\n",
    "        outcomes for the game + len(gameId, 2 lineups, ah line) .\n",
    "\n",
    "    action_space : gym.spaces.Discrete\n",
    "        The action space for the environment.\n",
    "        The action space is a set of choices that the agent can do.\n",
    "\n",
    "    balance : float\n",
    "        The current balance of the environment.\n",
    "\n",
    "    starting_bank : int, default=100\n",
    "        The starting bank / balance for the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    # actions\n",
    "    ACTIONS_LIST = [\n",
    "        [0, 0, 0, 0, 0],  # no bets\n",
    "        [0.05, 0, 0, 0, 0],  # betting on home team (1x2)\n",
    "        [0.4, 0, 0, 0, 0],  # betting on home team (1x2)\n",
    "        [0.7, 0, 0, 0, 0],  # betting on home team (1x2)\n",
    "        [0, 0, 0.05, 0, 0],  # betting on away team (1x2)\n",
    "        [0, 0, 0.4, 0, 0],  # betting on away team (1x2)\n",
    "        [0, 0, 0.7, 0, 0],  # betting on away team (1x2)\n",
    "        [0, 0.05, 0, 0, 0],  # betting on draw (1x2)\n",
    "        [0, 0.4, 0, 0, 0],  # betting on draw (1x2)\n",
    "        [0, 0.7, 0, 0, 0],  # betting on draw (1x2)\n",
    "        [0, 0, 0, 0.05, 0],  # betting on home (Asian Handicap)\n",
    "        [0, 0, 0, 0.4, 0],  # betting on home (Asian Handicap)\n",
    "        [0, 0, 0, 0.7, 0],  # betting on home (Asian Handicap)\n",
    "        [0, 0, 0, 0, 0.05],  # betting on away (Asian Handicap)\n",
    "        [0, 0, 0, 0, 0.4],  # betting on away (Asian Handicap)\n",
    "        [0, 0, 0, 0, 0.7],  # betting on away (Asian Handicap)\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_odds,\n",
    "        odds_column_names=[\n",
    "            \"preGameOdds1\",\n",
    "            \"preGameOdds2\",\n",
    "            \"preGameOddsX\",\n",
    "            \"preGameAhHome\",\n",
    "            \"preGameAhAway\",\n",
    "        ],\n",
    "        starting_bank=100,\n",
    "    ):\n",
    "        \"\"\"Initializes a new environment\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        game_odds: pandas dataframe\n",
    "            A list of games, with their betting odds.\n",
    "        odds_column_names: list of str\n",
    "            A list of column names with length == number of odds.\n",
    "        bet_size: list\n",
    "            3 possible bets : small, medium and large\n",
    "        starting_bank: int\n",
    "            bank account\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # games df\n",
    "        self._game = game_odds.copy()\n",
    "        # sort data by date\n",
    "        if \"gameDate\" in self._game.columns:\n",
    "            self._game[\"gameDate\"] = pd.to_datetime(self._game[\"gameDate\"])\n",
    "            self._game = self._game.sort_values(by=\"gameDate\")\n",
    "        # odds (1X2 and Asian handicap) values\n",
    "        self._odds = self._game[odds_column_names].values\n",
    "        # results\n",
    "        self._results = self._game[\"result\"].values\n",
    "        # ah lines\n",
    "        self._lines = self._game[\"lineId\"].values\n",
    "        # game goal-difference\n",
    "        self._gd = self._game[\"postGameGd\"].values\n",
    "        # teams names\n",
    "        self._teams_names = self._game[[\"homeTeamName\", \"awayTeamName\"]]\n",
    "        # teams opta id\n",
    "        self._teams_ids = self._game[[\"homeTeamOptaId\", \"awayTeamOptaId\"]].values\n",
    "\n",
    "        # teams lineups (names and positions)\n",
    "        self._lineups = self._game[[\"homeTeamLineup\", \"awayTeamLineup\"]].values\n",
    "        # teams lineups (opta ids)\n",
    "        self._lineups_ids = self._game[\n",
    "            [\"homeTeamLineupIds\", \"awayTeamLineupIds\"]\n",
    "        ].values\n",
    "        # games ids\n",
    "        self._game_ids = self._game[\"optaGameId\"].values\n",
    "        # observation space\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(\n",
    "                self._odds.shape[1] + 25,\n",
    "            ),  # 25 = 22(players Ids) + 2(home and away team ids) + 1(gameId)\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "        # actions space\n",
    "        self.action_space = gym.spaces.Discrete(\n",
    "            len(BettingEnv.ACTIONS_LIST)\n",
    "        )  # betting action\n",
    "        # env balance\n",
    "        self.balance = self.starting_bank = starting_bank\n",
    "        # current step (game)\n",
    "        self.current_step = 0\n",
    "        # bet size for each outcome\n",
    "        self.bet_size_matrix = None\n",
    "\n",
    "    def _get_current_index(self):\n",
    "        return self.current_step % self._odds.shape[0]\n",
    "\n",
    "    def get_odds(self):\n",
    "        \"\"\"Returns the odds for the current step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        odds : numpy.ndarray of shape (1, n_odds)\n",
    "            The odds for the current step.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame([self._odds[self.current_step]]).values\n",
    "\n",
    "    def get_bet(self, action):\n",
    "        \"\"\"Returns the betting matrix for the action provided.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action provided by the agent.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bet : array of shape (1, n_odds)\n",
    "            The betting matrix, where each outcome specified in the action\n",
    "            has a value of 1 and 0 otherwise.\n",
    "        \"\"\"\n",
    "        return BettingEnv.ACTIONS_LIST[action]\n",
    "\n",
    "    @pretty_traceback_and_exit_decorator\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of episode is reached,\n",
    "        you are responsible for calling reset() to reset this environment's state.\n",
    "\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action provided by the agent.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        observation : dataframe\n",
    "            The agent's observation of the current environment\n",
    "        reward : float\n",
    "            The amount of reward returned after previous action\n",
    "        done : bool\n",
    "            Whether the episode has ended, in which case further step() calls will return undefined results\n",
    "        info : dict\n",
    "            Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "\n",
    "        \"\"\"\n",
    "        # init observation\n",
    "        observation = np.ones(shape=self.observation_space.shape)\n",
    "        # reward\n",
    "        reward = 0\n",
    "        # finish\n",
    "        done = False\n",
    "        # episode info\n",
    "        info = self.create_info(action)\n",
    "        \n",
    "        if self.balance < 1:  # no more money\n",
    "            done = True\n",
    "        else:\n",
    "            # bet action\n",
    "            bet = self.get_bet(action)\n",
    "            # game result\n",
    "            results = self.get_results()\n",
    "            if self.legal_bet(bet):  # making sure agent has enough money for the bet\n",
    "                # current odds\n",
    "                odds = self.get_odds()\n",
    "                # reward (positive or negative)\n",
    "                reward = self.get_reward(bet, odds, results)\n",
    "                # update balance\n",
    "                self.balance += reward\n",
    "                info.update(legal_bet=True)\n",
    "            else:\n",
    "                reward = -(bet * self.bet_size_matrix).sum()\n",
    "            # update info\n",
    "            info.update(results=results.argmax())\n",
    "            info.update(reward=reward)\n",
    "            # increment step\n",
    "            self.current_step += 1\n",
    "            # check if we are finished\n",
    "            if self.finish():\n",
    "                done = True\n",
    "            else:\n",
    "                observation = self.get_observation()\n",
    "        \n",
    "        # update flag\n",
    "        info.update(done=done)\n",
    "        # return\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"return the observation of the current step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        obs : numpy.ndarray of shape (1, n_odds + 22)\n",
    "            The observation of the current step.\n",
    "        \"\"\"\n",
    "        # current game\n",
    "        index = self._get_current_index()\n",
    "        # current game id\n",
    "        game_id = self._game_ids[index]\n",
    "        # current game lineups\n",
    "        lineups = self._lineups[index]\n",
    "        lineups_ids = self._lineups_ids[index]\n",
    "        # teams\n",
    "        teams_names = self._teams_names.iloc[index]\n",
    "        teams_ids = self._teams_ids[index]\n",
    "        # 1X2 and AH odds\n",
    "        betting_market = self.get_odds()\n",
    "        # chosen line (AH line)\n",
    "        ah_line = self._lines[index]\n",
    "\n",
    "        # observation\n",
    "        observation = Observation(\n",
    "            game_id,\n",
    "            lineups,\n",
    "            lineups_ids,\n",
    "            teams_names,\n",
    "            teams_ids,\n",
    "            betting_market,\n",
    "            ah_line,\n",
    "        )\n",
    "        observation = observation().reshape(self.observation_space.shape)\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def get_reward(self, bet, odds, results):\n",
    "        \"\"\"Calculates the reward\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bet : array of shape (1, n_odds)\n",
    "        odds: dataframe of shape (1, n_odds)\n",
    "            A games with its betting odds.\n",
    "        results : array of shape (1, n_odds)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward : float\n",
    "            The amount of reward returned after previous action\n",
    "        \"\"\"\n",
    "        # agent choice\n",
    "        bet_index = np.argmax(np.array(bet))\n",
    "        # bet size\n",
    "        bet_size_matrix = self.bet_size_matrix\n",
    "        # balance\n",
    "        balance = self.balance\n",
    "        # if the action is a AH bet\n",
    "        if bet_index in [3, 4]:\n",
    "            # game goal_difference\n",
    "            obs_gd = (\n",
    "                self._gd[self.current_step]\n",
    "                if bet_index == 3\n",
    "                else -self._gd[self.current_step]\n",
    "            )\n",
    "            # ah line\n",
    "            ah_line = float(\n",
    "                self._lines[self.current_step]\n",
    "                if bet_index == 3\n",
    "                else -self._lines[self.current_step]\n",
    "            )\n",
    "            # ah side odds\n",
    "            ah_odds = (\n",
    "                odds[:, 3:4][0].item() if bet_index == 3 else odds[:, 4:][0].item()\n",
    "            )\n",
    "            # calculate profit\n",
    "            profit = AsianHandicap.pnl(obs_gd, ah_line, ah_odds)\n",
    "            profit = (\n",
    "                0 if profit is None else numexpr.evaluate(\"sum(bet * balance * profit)\")\n",
    "            )\n",
    "        else:  # case 1X2\n",
    "            reward = numexpr.evaluate(\"sum(bet * balance * results * odds)\")\n",
    "            expense = numexpr.evaluate(\"sum(bet * balance)\")\n",
    "            profit = reward - expense\n",
    "\n",
    "        return profit\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        observation : dataframe\n",
    "            the initial observation.\n",
    "        \"\"\"\n",
    "        self.balance = self.starting_bank\n",
    "        self.current_step = 0\n",
    "        return self.get_observation()\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Outputs the current balance and the current step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        msg : str\n",
    "            A string with the current balance,\n",
    "            the current step and the current game info.\n",
    "        \"\"\"\n",
    "        index = self._get_current_index()\n",
    "        teams = self._teams_names.iloc[index]\n",
    "        game_id = self._game_ids[index]\n",
    "        teams = teams.itertuples() if isinstance(teams, pd.DataFrame) else [teams]\n",
    "        teams_str = \", \".join(\n",
    "            [\n",
    "                \"Home Team: {} VS Away Team: {}\".format(\n",
    "                    row.homeTeamName, row.awayTeamName\n",
    "                )\n",
    "                for row in teams\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"Current balance at step {}: {}\".format(self.current_step, self.balance))\n",
    "        print(\"Current game id : {}\".format(game_id))\n",
    "        print(teams_str)\n",
    "\n",
    "    def finish(self):\n",
    "        \"\"\"Checks if the episode has reached an end.\n",
    "\n",
    "        The episode has reached an end if there are no more games to bet.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        finish : bool\n",
    "            True if the current_step is equal to n_games, False otherwise\n",
    "        \"\"\"\n",
    "        return self.current_step == self._odds.shape[0]  # no more games left to bet\n",
    "\n",
    "    def get_results(self):\n",
    "        \"\"\"Returns the results matrix for the current step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result : array of shape (1, n_odds)\n",
    "            The result matrix, where the index of the outcome that happened\n",
    "            value is 1 and the rest of the indexes values are 0.\n",
    "        \"\"\"\n",
    "        result = np.zeros(shape=(1, self._odds.shape[1]))\n",
    "        result[\n",
    "            np.arange(result.shape[0], dtype=np.int32),\n",
    "            np.array([self._results[self.current_step]], dtype=np.int32),\n",
    "        ] = 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    def legal_bet(self, bet):\n",
    "        \"\"\"Checks if the bet is legal.\n",
    "\n",
    "        Checks that the bet does not exceed the current balance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bet : array of shape (1, n_odds)\n",
    "            The bet to check.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        legal : bool\n",
    "            True if the bet is legal, False otherwise.\n",
    "        \"\"\"\n",
    "        bet_size = sum([b * self.balance for b in bet])\n",
    "        return bet_size <= self.balance\n",
    "\n",
    "    def create_info(self, action):\n",
    "        \"\"\"Creates the info dictionary for the given action.\n",
    "\n",
    "        The info dictionary holds the following information:\n",
    "            * the current step\n",
    "            * game odds of the current step\n",
    "            * bet action of the current step\n",
    "            * bet size of the current step\n",
    "            * the balance at the start of the current step\n",
    "            * reward of the current step\n",
    "            * game result of the current step\n",
    "            * state of the current step\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action provided by the agent.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        info : dict\n",
    "            The info dictionary.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"current_step\": self.current_step,\n",
    "            \"odds\": self.get_odds(),\n",
    "            \"bet_action\": env.ACTIONS_LIST[action],\n",
    "            \"balance\": self.balance,\n",
    "            \"reward\": 0,\n",
    "            \"legal_bet\": False,\n",
    "            \"results\": None,\n",
    "            \"done\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load games data from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "\n",
    "# load data\n",
    "raw_odds_data = pd.read_csv(\"Hosts_edges.csv\").head(5)\n",
    "# extract specific fields\n",
    "odds_dataframe = raw_odds_data[\n",
    "    [\n",
    "        \"gameId\",\n",
    "        \"gameDate\",\n",
    "        \"homeTeamId\",\n",
    "        \"homeTeamName\",\n",
    "        \"awayTeamId\",\n",
    "        \"awayTeamName\",\n",
    "        \"preGame_odds1\",\n",
    "        \"preGame_oddsX\",\n",
    "        \"preGame_odds2\",\n",
    "    ]\n",
    "].sort_values(by=\"gameDate\")\n",
    "# change columns names\n",
    "odds_dataframe.rename(\n",
    "    columns={\n",
    "        \"preGame_odds1\": \"preGameOdds1\",\n",
    "        \"preGame_oddsX\": \"preGameOddsX\",\n",
    "        \"preGame_odds2\": \"preGameOdds2\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "\n",
    "# initialise connections\n",
    "mongo_init(\"prod_atlas\")\n",
    "\n",
    "# add opta game Id\n",
    "odds_dataframe[\"optaGameId\"] = odds_dataframe.apply(\n",
    "    lambda row: GameFeatures.get_game(row[\"gameId\"]).game_opta_id,\n",
    "    axis=\"columns\",\n",
    ")\n",
    "# add home team and away team opta Ids\n",
    "odds_dataframe[\"homeTeamOptaId\"] = odds_dataframe.apply(\n",
    "    lambda row: GameFeatures.get_game(row[\"gameId\"]).ht_opta_id,\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "odds_dataframe[\"awayTeamOptaId\"] = odds_dataframe.apply(\n",
    "    lambda row: GameFeatures.get_game(row[\"gameId\"]).at_opta_id,\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "\n",
    "# add asian handicap\n",
    "odds_dataframe[\"preGameAhHome\"] = odds_dataframe.apply(\n",
    "    lambda row: MarketOdds.get_latest(row[\"gameId\"], \"asian\")[\"odds1\"][0],\n",
    "    axis=\"columns\",\n",
    "    result_type=\"expand\",\n",
    ")\n",
    "odds_dataframe[\"preGameAhAway\"] = odds_dataframe.apply(\n",
    "    lambda row: MarketOdds.get_latest(row[\"gameId\"], \"asian\")[\"odds2\"][0],\n",
    "    axis=\"columns\",\n",
    "    result_type=\"expand\",\n",
    ")\n",
    "odds_dataframe[\"lineId\"] = odds_dataframe.apply(\n",
    "    lambda row: MarketOdds.get_latest(row[\"gameId\"], \"asian\")[\"line_id\"][0],\n",
    "    axis=\"columns\",\n",
    "    result_type=\"expand\",\n",
    ")\n",
    "# add home team lineup\n",
    "odds_dataframe[\"homeTeamLineup\"] = odds_dataframe.apply(\n",
    "    lambda row: json.dumps(\n",
    "        {\n",
    "            player.name: player.position\n",
    "            for player in TeamSheet.get_latest(\n",
    "                ra_team_id=row[\"homeTeamId\"], date=row[\"gameDate\"]\n",
    "            ).starting\n",
    "        }\n",
    "    ),\n",
    "    axis=\"columns\",\n",
    "    result_type=\"expand\",\n",
    ")\n",
    "odds_dataframe[\"homeTeamLineupIds\"] = odds_dataframe.apply(\n",
    "    lambda row: list(\n",
    "        player.opta_id\n",
    "        for player in TeamSheet.get_latest(\n",
    "            ra_team_id=row[\"homeTeamId\"], date=row[\"gameDate\"]\n",
    "        ).starting\n",
    "    ),\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "# add away team lineup\n",
    "odds_dataframe[\"awayTeamLineup\"] = odds_dataframe.apply(\n",
    "    lambda row: json.dumps(\n",
    "        {\n",
    "            player.name: player.position\n",
    "            for player in TeamSheet.get_latest(\n",
    "                ra_team_id=row[\"awayTeamId\"], date=row[\"gameDate\"]\n",
    "            ).starting\n",
    "        }\n",
    "    ),\n",
    "    axis=\"columns\",\n",
    "    result_type=\"expand\",\n",
    ")\n",
    "\n",
    "odds_dataframe[\"awayTeamLineupIds\"] = odds_dataframe.apply(\n",
    "    lambda row: list(\n",
    "        player.opta_id\n",
    "        for player in TeamSheet.get_latest(\n",
    "            ra_team_id=row[\"awayTeamId\"], date=row[\"gameDate\"]\n",
    "        ).starting\n",
    "    ),\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "# map results {homewin -> 0 , draw -> 1, awaywin -> 2}\n",
    "odds_dataframe[\"result\"] = raw_odds_data[\"postGame_tgt_outcome\"].map(\n",
    "    {1.0: 0.0, 0.0: 2.0, 0.5: 1.0}\n",
    ")\n",
    "# gd results\n",
    "odds_dataframe[\"postGameGd\"] = raw_odds_data[\"postGame_tgt_gd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameId</th>\n",
       "      <th>gameDate</th>\n",
       "      <th>homeTeamId</th>\n",
       "      <th>homeTeamName</th>\n",
       "      <th>awayTeamId</th>\n",
       "      <th>awayTeamName</th>\n",
       "      <th>preGameOdds1</th>\n",
       "      <th>preGameOddsX</th>\n",
       "      <th>preGameOdds2</th>\n",
       "      <th>optaGameId</th>\n",
       "      <th>...</th>\n",
       "      <th>awayTeamOptaId</th>\n",
       "      <th>preGameAhHome</th>\n",
       "      <th>preGameAhAway</th>\n",
       "      <th>lineId</th>\n",
       "      <th>homeTeamLineup</th>\n",
       "      <th>homeTeamLineupIds</th>\n",
       "      <th>awayTeamLineup</th>\n",
       "      <th>awayTeamLineupIds</th>\n",
       "      <th>result</th>\n",
       "      <th>postGameGd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0034edd948828f48e2b27ab340de6a86c4d53d04040bee...</td>\n",
       "      <td>2018-10-03T18:45:00.000Z</td>\n",
       "      <td>aeb2f56fcedbcf4cd5c780179766996c7bf0b308064541...</td>\n",
       "      <td>Blackburn Rovers</td>\n",
       "      <td>4a104655f366c090e1849b57b87890a68f9400c8dd4a6b...</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.55</td>\n",
       "      <td>2.22</td>\n",
       "      <td>991086</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.25</td>\n",
       "      <td>{\"David Raya Martin\": \"GK\", \"Elliott Bennett\":...</td>\n",
       "      <td>[154561, 41262, 165183, 87115, 167802, 110403,...</td>\n",
       "      <td>{\"Dean Henderson\": \"GK\", \"Kieron Freeman\": \"MR...</td>\n",
       "      <td>[172649, 87121, 63426, 146610, 108416, 40386, ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001597ea24d591c49f1d89e9f415042a82c78f2b87bda3...</td>\n",
       "      <td>2018-11-25T20:00:00.000Z</td>\n",
       "      <td>6673572f126d843a06ec48ebf5ed8fcd8234bee469bb11...</td>\n",
       "      <td>Amiens</td>\n",
       "      <td>9551136312d3b08191d8813198a3486995f3b212ff1e01...</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>4.85</td>\n",
       "      <td>3.47</td>\n",
       "      <td>1.88</td>\n",
       "      <td>985289</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.50</td>\n",
       "      <td>{\"Regis Gurtner\": \"GK\", \"Emil Krafth\": \"DR\", \"...</td>\n",
       "      <td>[42766, 111773, 228289, 201392, 98798, 220960,...</td>\n",
       "      <td>{\"Steve Mandanda\": \"GK\", \"Bouna Sarr\": \"MR\", \"...</td>\n",
       "      <td>[44413, 102760, 121117, 109404, 39294, 41795, ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004ee2a59c5642a08c80f4861155738e75fae5a9a8f040...</td>\n",
       "      <td>2018-12-18T19:30:00.000Z</td>\n",
       "      <td>cd4ba7e93005562480c86a8eaec77586bd6d4b534eeae9...</td>\n",
       "      <td>Hertha BSC</td>\n",
       "      <td>579831158152f80a0b90461b86193ba9132ce265690a98...</td>\n",
       "      <td>FC Augsburg</td>\n",
       "      <td>2.28</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.30</td>\n",
       "      <td>995373</td>\n",
       "      <td>...</td>\n",
       "      <td>1772</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.96</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>{\"Rune Jarstein\": \"GK\", \"Peter Pekarik\": \"DR\",...</td>\n",
       "      <td>[28087, 42614, 89484, 116543, 41854, 244761, 1...</td>\n",
       "      <td>{\"Andreas Luthe\": \"GK\", \"Jonathan Schmid\": \"DR...</td>\n",
       "      <td>[62366, 90514, 179597, 111450, 135720, 88033, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003a183e60ea92d1a01d39a673353f94007e02455acec0...</td>\n",
       "      <td>2019-04-20T18:00:00.000Z</td>\n",
       "      <td>54402207f254a5a270bcc95b53124165885b824c98fd3e...</td>\n",
       "      <td>Nîmes</td>\n",
       "      <td>21c36d309100f69db52126912d1a3678fe16d1aeee1183...</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>2.02</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.84</td>\n",
       "      <td>985484</td>\n",
       "      <td>...</td>\n",
       "      <td>140</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>{\"Paul Bernardoni\": \"GK\", \"Renaud Ripart\": \"DR...</td>\n",
       "      <td>[200848, 121150, 214473, 102636, 200666, 88036...</td>\n",
       "      <td>{\"Benoit Costil\": \"GK\", \"Francois Kamano\": \"MR...</td>\n",
       "      <td>[42713, 193512, 164474, 210228, 220325, 482549...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002b5b08be71f17a9bda4f789160a50e5cdf45b9e685d7...</td>\n",
       "      <td>2019-05-04T13:30:00.000Z</td>\n",
       "      <td>503fa80a790a8a8760f6a0acc887633335c4be1170caad...</td>\n",
       "      <td>FC Bayern München</td>\n",
       "      <td>2458b801f0667e51562faa1ce3d36d980d3bdb76f8941c...</td>\n",
       "      <td>Hannover 96</td>\n",
       "      <td>1.03</td>\n",
       "      <td>26.00</td>\n",
       "      <td>69.00</td>\n",
       "      <td>995514</td>\n",
       "      <td>...</td>\n",
       "      <td>808</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.85</td>\n",
       "      <td>-3.75</td>\n",
       "      <td>{\"Sven Ulreich\": \"GK\", \"Joshua Kimmich\": \"DR\",...</td>\n",
       "      <td>[49431, 165687, 50188, 119090, 161450, 40691, ...</td>\n",
       "      <td>{\"Michael Esser\": \"GK\", \"Oliver Sorg\": \"DR\", \"...</td>\n",
       "      <td>[84757, 111250, 61758, 37606, 212509, 52651, 5...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              gameId  \\\n",
       "2  0034edd948828f48e2b27ab340de6a86c4d53d04040bee...   \n",
       "0  001597ea24d591c49f1d89e9f415042a82c78f2b87bda3...   \n",
       "4  004ee2a59c5642a08c80f4861155738e75fae5a9a8f040...   \n",
       "3  003a183e60ea92d1a01d39a673353f94007e02455acec0...   \n",
       "1  002b5b08be71f17a9bda4f789160a50e5cdf45b9e685d7...   \n",
       "\n",
       "                   gameDate  \\\n",
       "2  2018-10-03T18:45:00.000Z   \n",
       "0  2018-11-25T20:00:00.000Z   \n",
       "4  2018-12-18T19:30:00.000Z   \n",
       "3  2019-04-20T18:00:00.000Z   \n",
       "1  2019-05-04T13:30:00.000Z   \n",
       "\n",
       "                                          homeTeamId       homeTeamName  \\\n",
       "2  aeb2f56fcedbcf4cd5c780179766996c7bf0b308064541...   Blackburn Rovers   \n",
       "0  6673572f126d843a06ec48ebf5ed8fcd8234bee469bb11...             Amiens   \n",
       "4  cd4ba7e93005562480c86a8eaec77586bd6d4b534eeae9...         Hertha BSC   \n",
       "3  54402207f254a5a270bcc95b53124165885b824c98fd3e...              Nîmes   \n",
       "1  503fa80a790a8a8760f6a0acc887633335c4be1170caad...  FC Bayern München   \n",
       "\n",
       "                                          awayTeamId      awayTeamName  \\\n",
       "2  4a104655f366c090e1849b57b87890a68f9400c8dd4a6b...  Sheffield United   \n",
       "0  9551136312d3b08191d8813198a3486995f3b212ff1e01...         Marseille   \n",
       "4  579831158152f80a0b90461b86193ba9132ce265690a98...       FC Augsburg   \n",
       "3  21c36d309100f69db52126912d1a3678fe16d1aeee1183...          Bordeaux   \n",
       "1  2458b801f0667e51562faa1ce3d36d980d3bdb76f8941c...       Hannover 96   \n",
       "\n",
       "   preGameOdds1  preGameOddsX  preGameOdds2  optaGameId  ...  awayTeamOptaId  \\\n",
       "2          3.41          3.55          2.22      991086  ...              49   \n",
       "0          4.85          3.47          1.88      985289  ...             144   \n",
       "4          2.28          3.25          3.30      995373  ...            1772   \n",
       "3          2.02          3.69          3.84      985484  ...             140   \n",
       "1          1.03         26.00         69.00      995514  ...             808   \n",
       "\n",
       "   preGameAhHome  preGameAhAway  lineId  \\\n",
       "2           2.02           1.90    0.25   \n",
       "0           2.04           1.88    0.50   \n",
       "4           1.99           1.96   -0.25   \n",
       "3           1.72           2.25   -0.25   \n",
       "1           2.08           1.85   -3.75   \n",
       "\n",
       "                                      homeTeamLineup  \\\n",
       "2  {\"David Raya Martin\": \"GK\", \"Elliott Bennett\":...   \n",
       "0  {\"Regis Gurtner\": \"GK\", \"Emil Krafth\": \"DR\", \"...   \n",
       "4  {\"Rune Jarstein\": \"GK\", \"Peter Pekarik\": \"DR\",...   \n",
       "3  {\"Paul Bernardoni\": \"GK\", \"Renaud Ripart\": \"DR...   \n",
       "1  {\"Sven Ulreich\": \"GK\", \"Joshua Kimmich\": \"DR\",...   \n",
       "\n",
       "                                   homeTeamLineupIds  \\\n",
       "2  [154561, 41262, 165183, 87115, 167802, 110403,...   \n",
       "0  [42766, 111773, 228289, 201392, 98798, 220960,...   \n",
       "4  [28087, 42614, 89484, 116543, 41854, 244761, 1...   \n",
       "3  [200848, 121150, 214473, 102636, 200666, 88036...   \n",
       "1  [49431, 165687, 50188, 119090, 161450, 40691, ...   \n",
       "\n",
       "                                      awayTeamLineup  \\\n",
       "2  {\"Dean Henderson\": \"GK\", \"Kieron Freeman\": \"MR...   \n",
       "0  {\"Steve Mandanda\": \"GK\", \"Bouna Sarr\": \"MR\", \"...   \n",
       "4  {\"Andreas Luthe\": \"GK\", \"Jonathan Schmid\": \"DR...   \n",
       "3  {\"Benoit Costil\": \"GK\", \"Francois Kamano\": \"MR...   \n",
       "1  {\"Michael Esser\": \"GK\", \"Oliver Sorg\": \"DR\", \"...   \n",
       "\n",
       "                                   awayTeamLineupIds result  postGameGd  \n",
       "2  [172649, 87121, 63426, 146610, 108416, 40386, ...    2.0          -2  \n",
       "0  [44413, 102760, 121117, 109404, 39294, 41795, ...    2.0          -2  \n",
       "4  [62366, 90514, 179597, 111450, 135720, 88033, ...    1.0           0  \n",
       "3  [42713, 193512, 164474, 210228, 220325, 482549...    0.0           1  \n",
       "1  [84757, 111250, 61758, 37606, 212509, 52651, 5...    0.0           2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | include: false\n",
    "odds_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent - Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll set up our betting environment and let the computer program play and make decisions at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "env = BettingEnv(odds_dataframe.reset_index())\n",
    "max_steps_limit = odds_dataframe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current balance at step 0: 100\n",
      "Current game id : 991086\n",
      "Home Team: Blackburn Rovers VS Away Team: Sheffield United\n",
      "None\n",
      "\n",
      " Info: \n",
      "\n",
      "{'current_step': 0, 'odds': array([[3.41, 2.22, 3.55, 2.02, 1.9 ]]), 'bet_action': [0.05, 0, 0, 0, 0], 'balance': 100, 'reward': -5.0, 'legal_bet': True, 'results': 2, 'done': False}\n",
      "\n",
      " Observation: \n",
      "\n",
      "[9.85289e+05 1.43000e+03 1.44000e+02 4.27660e+04 1.11773e+05 2.28289e+05\n",
      " 2.01392e+05 9.87980e+04 2.20960e+05 1.11181e+05 9.66160e+04 2.05836e+05\n",
      " 1.92785e+05 1.16152e+05 4.44130e+04 1.02760e+05 1.21117e+05 1.09404e+05\n",
      " 3.92940e+04 4.17950e+04 2.13965e+05 1.22775e+05 1.45212e+05 9.54340e+04\n",
      " 3.79010e+04 4.85000e+00 1.88000e+00 3.47000e+00 2.04000e+00 1.88000e+00]\n",
      "----------------------------------------------------\n",
      "Current balance at step 1: 95.0\n",
      "Current game id : 985289\n",
      "Home Team: Amiens VS Away Team: Marseille\n",
      "None\n",
      "\n",
      " Info: \n",
      "\n",
      "{'current_step': 1, 'odds': array([[4.85, 1.88, 3.47, 2.04, 1.88]]), 'bet_action': [0, 0.7, 0, 0, 0], 'balance': 95.0, 'reward': -66.5, 'legal_bet': True, 'results': 2, 'done': False}\n",
      "\n",
      " Observation: \n",
      "\n",
      "[9.95373e+05 1.62000e+02 1.77200e+03 2.80870e+04 4.26140e+04 8.94840e+04\n",
      " 1.16543e+05 4.18540e+04 2.44761e+05 1.00887e+05 2.19848e+05 1.74657e+05\n",
      " 1.39110e+05 2.04335e+05 6.23660e+04 9.05140e+04 1.79597e+05 1.11450e+05\n",
      " 1.35720e+05 8.80330e+04 7.78650e+04 6.24800e+04 6.29800e+04 1.71210e+04\n",
      " 9.87690e+04 2.28000e+00 3.30000e+00 3.25000e+00 1.99000e+00 1.96000e+00]\n",
      "----------------------------------------------------\n",
      "Current balance at step 2: 28.5\n",
      "Current game id : 995373\n",
      "Home Team: Hertha BSC VS Away Team: FC Augsburg\n",
      "None\n",
      "\n",
      " Info: \n",
      "\n",
      "{'current_step': 2, 'odds': array([[2.28, 3.3 , 3.25, 1.99, 1.96]]), 'bet_action': [0, 0, 0, 0.05, 0], 'balance': 28.5, 'reward': array(-0.7125), 'legal_bet': True, 'results': 1, 'done': False}\n",
      "\n",
      " Observation: \n",
      "\n",
      "[9.85484e+05 2.33600e+03 1.40000e+02 2.00848e+05 1.21150e+05 2.14473e+05\n",
      " 1.02636e+05 2.00666e+05 8.80360e+04 2.16227e+05 1.07641e+05 2.46451e+05\n",
      " 1.02743e+05 1.98990e+05 4.27130e+04 1.93512e+05 1.64474e+05 2.10228e+05\n",
      " 2.20325e+05 4.82549e+05 1.80835e+05 2.12851e+05 1.95480e+05 2.44182e+05\n",
      " 7.38790e+04 2.02000e+00 3.84000e+00 3.69000e+00 1.72000e+00 2.25000e+00]\n",
      "----------------------------------------------------\n",
      "Current balance at step 3: 27.7875\n",
      "Current game id : 985484\n",
      "Home Team: Nîmes VS Away Team: Bordeaux\n",
      "None\n",
      "\n",
      " Info: \n",
      "\n",
      "{'current_step': 3, 'odds': array([[2.02, 3.84, 3.69, 1.72, 2.25]]), 'bet_action': [0, 0, 0, 0, 0.4], 'balance': 27.7875, 'reward': array(-11.115), 'legal_bet': True, 'results': 0, 'done': False}\n",
      "\n",
      " Observation: \n",
      "\n",
      "[9.95514e+05 1.56000e+02 8.08000e+02 4.94310e+04 1.65687e+05 5.01880e+04\n",
      " 1.19090e+05 1.61450e+05 4.06910e+04 1.33798e+05 6.15580e+04 5.67640e+04\n",
      " 5.56340e+04 1.29508e+05 8.47570e+04 1.11250e+05 6.17580e+04 3.76060e+04\n",
      " 2.12509e+05 5.26510e+04 5.40780e+04 1.84289e+05 2.30306e+05 2.19846e+05\n",
      " 9.21740e+04 1.03000e+00 6.90000e+01 2.60000e+01 2.08000e+00 1.85000e+00]\n",
      "----------------------------------------------------\n",
      "Current balance at step 4: 16.6725\n",
      "Current game id : 995514\n",
      "Home Team: FC Bayern München VS Away Team: Hannover 96\n",
      "None\n",
      "\n",
      " Info: \n",
      "\n",
      "{'current_step': 4, 'odds': array([[ 1.03, 69.  , 26.  ,  2.08,  1.85]]), 'bet_action': [0, 0, 0, 0.4, 0], 'balance': 16.6725, 'reward': array(-6.669), 'legal_bet': True, 'results': 0, 'done': True}\n",
      "\n",
      " Observation: \n",
      "\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# | include: false\n",
    "env.reset()\n",
    "for _ in range(0, max_steps_limit):\n",
    "    print(env.render())\n",
    "    print(\"\\n Info: \\n\")\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(info)\n",
    "    print(\"\\n Observation: \\n\")\n",
    "    print(obs)\n",
    "    print(\"----------------------------------------------------\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test environment with D3rlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from d3rlpy import torch_utility\n",
    "from d3rlpy.torch_utility import _WithDeviceAndScalerProtocol, TorchMiniBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function\n",
    "In order to apply the monkey-patching concept, we have to set a function that reload all affected packages and modules. To do so, we need to delete them from memory cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def uncache(exclude):\n",
    "    \"\"\"\n",
    "    Remove package modules from cache except excluded ones.\n",
    "    On next import they will be reloaded.\n",
    "    \n",
    "    Args:\n",
    "        exclude (iter<str>): Sequence of module paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    pkgs = []\n",
    "    for mod in exclude:\n",
    "        pkg = mod.split('.', 1)[0]\n",
    "        pkgs.append(pkg)\n",
    "\n",
    "    to_uncache = []\n",
    "    for mod in sys.modules:\n",
    "        if mod in exclude:\n",
    "            continue\n",
    "\n",
    "        if mod in pkgs:\n",
    "            to_uncache.append(mod)\n",
    "            continue\n",
    "\n",
    "        for pkg in pkgs:\n",
    "            if mod.startswith(pkg + '.'):\n",
    "                to_uncache.append(mod)\n",
    "                break\n",
    "\n",
    "    for mod in to_uncache:\n",
    "        del sys.modules[mod]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will override this function from the d3rlpy package and adapt it to our use case.\n",
    "Here we will add the Observation output instance in the condition part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def torch_api(\n",
    "    scaler_targets: Optional[List[str]] = None,\n",
    "    action_scaler_targets: Optional[List[str]] = None,\n",
    "    reward_scaler_targets: Optional[List[str]] = None,\n",
    ") -> Callable[..., np.ndarray]:\n",
    "    def _torch_api(f: Callable[..., np.ndarray]) -> Callable[..., np.ndarray]:\n",
    "        # get argument names\n",
    "        sig = signature(f)\n",
    "        arg_keys = list(sig.parameters.keys())[1:]\n",
    "        \n",
    "        def wrapper(\n",
    "            self: _WithDeviceAndScalerProtocol, *args: Any, **kwargs: Any\n",
    "        ) -> np.ndarray:\n",
    "            tensors: List[Union[torch.Tensor, TorchMiniBatch]] = []\n",
    "            # convert all args to torch.Tensor\n",
    "            for i, val in enumerate(args):\n",
    "                tensor: Union[torch.Tensor, TorchMiniBatch]\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    tensor = val\n",
    "                elif isinstance(val, list):\n",
    "                    tensor = default_collate(val)\n",
    "                    tensor = tensor.to(self.device)\n",
    "                elif isinstance(val, np.ndarray) or isinstance(val, Observation):\n",
    "                    if val.dtype == np.uint8:\n",
    "                        dtype = torch.uint8\n",
    "                    else:\n",
    "                        dtype = torch.float32\n",
    "                    tensor = torch.tensor(\n",
    "                        data=val,\n",
    "                        dtype=dtype,\n",
    "                        device=self.device,\n",
    "                    )\n",
    "                elif val is None:\n",
    "                    tensor = None\n",
    "                elif isinstance(val, TransitionMiniBatch):\n",
    "                    tensor = TorchMiniBatch(\n",
    "                        val,\n",
    "                        self.device,\n",
    "                        scaler=self.scaler,\n",
    "                        action_scaler=self.action_scaler,\n",
    "                        reward_scaler=self.reward_scaler,\n",
    "                    )\n",
    "                else:\n",
    "                    tensor = torch.tensor(\n",
    "                        data=val,\n",
    "                        dtype=torch.float32,\n",
    "                        device=self.device,\n",
    "                    )\n",
    "\n",
    "                if isinstance(tensor, torch.Tensor) or isinstance(val, Observation):\n",
    "                    # preprocess\n",
    "                    if self.scaler and scaler_targets:\n",
    "                        if arg_keys[i] in scaler_targets:\n",
    "                            tensor = self.scaler.transform(tensor)\n",
    "\n",
    "                    # preprocess action\n",
    "                    if self.action_scaler and action_scaler_targets:\n",
    "                        if arg_keys[i] in action_scaler_targets:\n",
    "                            tensor = self.action_scaler.transform(tensor)\n",
    "\n",
    "                    # preprocessing reward\n",
    "                    if self.reward_scaler and reward_scaler_targets:\n",
    "                        if arg_keys[i] in reward_scaler_targets:\n",
    "                            tensor = self.reward_scaler.transform(tensor)\n",
    "\n",
    "                    # make sure if the tensor is float32 type\n",
    "                    if tensor is not None and tensor.dtype != torch.float32:\n",
    "                        tensor = tensor.float()\n",
    "\n",
    "                tensors.append(tensor)\n",
    "            return f(self, *tensors, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return _torch_api\n",
    "\n",
    "\n",
    "torch_utility.torch_api = torch_api\n",
    "uncache([\"d3rlpy.torch_utility\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can monkey-patch the original function with our new implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from d3rlpy.algos import DQN\n",
    "from torch.optim import Adam\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.models.optimizers import OptimizerFactory\n",
    "from d3rlpy.preprocessing.scalers import Scaler, register_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this CustomScaler, we can do all the transformations that we want to apply on our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CustomScaler(Scaler):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_with_env(self, env: gym.Env):\n",
    "        pass\n",
    "\n",
    "    def transform(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # add numerical data to observations\n",
    "        observations = x\n",
    "        return observations\n",
    "\n",
    "    def reverse_transform(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def get_params(self, deep: bool = False) -> Dict[str, Any]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "register_scaler(CustomScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D3rlpy provides not only offline training, but also online training utilities. Here the buffer will try different experiences to collect a decent dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "buffer = ReplayBuffer(maxlen= 1000000, env= env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the time, the epsilon-greedy strategy chooses the action with the highest estimated reward. Exploration and exploitation should coexist in harmony. Exploration gives us the freedom to experiment with new ideas, often at contradiction with what we have already learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "\n",
    "# create the epsilon-greedy explorer\n",
    "explorer = LinearDecayEpsilonGreedy(start_epsilon=1.0,\n",
    "                                    end_epsilon=0.1,\n",
    "                                    duration=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer to update weights and reduce losses for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "\n",
    "# modify weight decay\n",
    "optim_factory = OptimizerFactory(Adam, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Implement the DQN Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | include: false\n",
    "\n",
    "custom_scaler = CustomScaler()\n",
    "\n",
    "dqn = DQN(\n",
    "    batch_size=32,\n",
    "    learning_rate=2.5e-4,\n",
    "    target_update_interval=100,\n",
    "    optim_factory=optim_factory,\n",
    "    scaler=custom_scaler,\n",
    ")\n",
    "\n",
    "dqn.build_with_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-21 00:16.18 [info     ] Directory is created at d3rlpy_logs/DQN_online_20230221001618\n",
      "2023-02-21 00:16.18 [debug    ] Fitting scaler...              scler=none\n",
      "2023-02-21 00:16.18 [warning  ] Skip building models since they're already built.\n",
      "2023-02-21 00:16.18 [info     ] Parameters are saved to d3rlpy_logs/DQN_online_20230221001618/params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 0.00025, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'weight_decay': 0.0001}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': {'type': 'none', 'params': {}}, 'target_update_interval': 100, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (30,), 'action_size': 16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303b4a243f7d4ecfa5dac773cc4523b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-21 00:16.18 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_100.pt\n",
      "2023-02-21 00:16.18 [info     ] DQN_online_20230221001618: epoch=1 step=100 epoch=1 metrics={'time_inference': 0.00030974388122558596, 'time_environment_step': 0.0005031299591064454, 'time_step': 0.0008710360527038574, 'rollout_return': -43.08417720138301} step=100\n",
      "2023-02-21 00:16.18 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_200.pt\n",
      "2023-02-21 00:16.18 [info     ] DQN_online_20230221001618: epoch=2 step=200 epoch=2 metrics={'time_inference': 0.00024736642837524413, 'time_environment_step': 0.0004901790618896484, 'time_sample_batch': 5.5711269378662106e-05, 'time_algorithm_update': 0.0012201762199401855, 'loss': 5468.900458984375, 'time_step': 0.0020819020271301268, 'rollout_return': 13.675259380999995} step=200\n",
      "2023-02-21 00:16.18 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_300.pt\n",
      "2023-02-21 00:16.18 [info     ] DQN_online_20230221001618: epoch=3 step=300 epoch=3 metrics={'time_inference': 0.00029319286346435547, 'time_environment_step': 0.0005681490898132324, 'time_sample_batch': 7.050752639770508e-05, 'time_algorithm_update': 0.001427900791168213, 'loss': 2352.4620098876953, 'time_step': 0.0024413609504699706, 'rollout_return': 2.586044685239989} step=300\n",
      "2023-02-21 00:16.19 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_400.pt\n",
      "2023-02-21 00:16.19 [info     ] DQN_online_20230221001618: epoch=4 step=400 epoch=4 metrics={'time_inference': 0.0003012561798095703, 'time_environment_step': 0.0006000661849975586, 'time_sample_batch': 7.626533508300781e-05, 'time_algorithm_update': 0.001473677158355713, 'loss': 1978.827041015625, 'time_step': 0.002541797161102295, 'rollout_return': -61.15536269679999} step=400\n",
      "2023-02-21 00:16.19 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_500.pt\n",
      "2023-02-21 00:16.19 [info     ] DQN_online_20230221001618: epoch=5 step=500 epoch=5 metrics={'time_inference': 0.00029044151306152345, 'time_environment_step': 0.0006126117706298828, 'time_sample_batch': 7.468938827514648e-05, 'time_algorithm_update': 0.0014476609230041505, 'loss': 1627.7948248291016, 'time_step': 0.0025109362602233885, 'rollout_return': -52.991275436574995} step=500\n",
      "2023-02-21 00:16.19 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_600.pt\n",
      "2023-02-21 00:16.19 [info     ] DQN_online_20230221001618: epoch=6 step=600 epoch=6 metrics={'time_inference': 0.00032738208770751956, 'time_environment_step': 0.0006788420677185059, 'time_sample_batch': 8.597850799560547e-05, 'time_algorithm_update': 0.0016028952598571777, 'loss': 1412.8559381103516, 'time_step': 0.002795290946960449, 'rollout_return': -10.734526904560003} step=600\n",
      "2023-02-21 00:16.19 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_700.pt\n",
      "2023-02-21 00:16.19 [info     ] DQN_online_20230221001618: epoch=7 step=700 epoch=7 metrics={'time_inference': 0.00029085159301757814, 'time_environment_step': 0.0006018519401550293, 'time_sample_batch': 8.287429809570313e-05, 'time_algorithm_update': 0.0014862990379333496, 'loss': 753.0497799682618, 'time_step': 0.00255263090133667, 'rollout_return': -46.989204826625} step=700\n",
      "2023-02-21 00:16.20 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_800.pt\n",
      "2023-02-21 00:16.20 [info     ] DQN_online_20230221001618: epoch=8 step=800 epoch=8 metrics={'time_inference': 0.0002585744857788086, 'time_environment_step': 0.0005098080635070801, 'time_sample_batch': 6.600379943847656e-05, 'time_algorithm_update': 0.0012686109542846679, 'loss': 718.4168539428711, 'time_step': 0.0021755337715148927, 'rollout_return': 44.46416867663999} step=800\n",
      "2023-02-21 00:16.20 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_900.pt\n",
      "2023-02-21 00:16.20 [info     ] DQN_online_20230221001618: epoch=9 step=900 epoch=9 metrics={'time_inference': 0.00027799129486083985, 'time_environment_step': 0.0005339407920837403, 'time_sample_batch': 7.060050964355469e-05, 'time_algorithm_update': 0.0013635540008544922, 'loss': 773.8450411987304, 'time_step': 0.002321605682373047, 'rollout_return': -7.203306650450003} step=900\n",
      "2023-02-21 00:16.20 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1000.pt\n",
      "2023-02-21 00:16.20 [info     ] DQN_online_20230221001618: epoch=10 step=1000 epoch=10 metrics={'time_inference': 0.0002836012840270996, 'time_environment_step': 0.0005697250366210937, 'time_sample_batch': 7.637500762939453e-05, 'time_algorithm_update': 0.0013907003402709962, 'loss': 571.059601135254, 'time_step': 0.0024067473411560057, 'rollout_return': -51.66232622950001} step=1000\n",
      "2023-02-21 00:16.20 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1100.pt\n",
      "2023-02-21 00:16.20 [info     ] DQN_online_20230221001618: epoch=11 step=1100 epoch=11 metrics={'time_inference': 0.0003148651123046875, 'time_environment_step': 0.0006299281120300293, 'time_sample_batch': 8.856534957885743e-05, 'time_algorithm_update': 0.001593489646911621, 'loss': 560.5489553833008, 'time_step': 0.002721550464630127, 'rollout_return': -0.41990614075000143} step=1100\n",
      "2023-02-21 00:16.21 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1200.pt\n",
      "2023-02-21 00:16.21 [info     ] DQN_online_20230221001618: epoch=12 step=1200 epoch=12 metrics={'time_inference': 0.000337522029876709, 'time_environment_step': 0.0006643104553222657, 'time_sample_batch': 9.102821350097657e-05, 'time_algorithm_update': 0.0015864300727844239, 'loss': 584.5428372192383, 'time_step': 0.002775576114654541, 'rollout_return': -14.130128638480002} step=1200\n",
      "2023-02-21 00:16.21 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1300.pt\n",
      "2023-02-21 00:16.21 [info     ] DQN_online_20230221001618: epoch=13 step=1300 epoch=13 metrics={'time_inference': 0.000233457088470459, 'time_environment_step': 0.00045656442642211915, 'time_sample_batch': 5.801916122436523e-05, 'time_algorithm_update': 0.0011790871620178222, 'loss': 613.9998330688477, 'time_step': 0.001997201442718506, 'rollout_return': -45.049765462150006} step=1300\n",
      "2023-02-21 00:16.21 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1400.pt\n",
      "2023-02-21 00:16.21 [info     ] DQN_online_20230221001618: epoch=14 step=1400 epoch=14 metrics={'time_inference': 0.0003221559524536133, 'time_environment_step': 0.0005921173095703125, 'time_sample_batch': 7.741212844848633e-05, 'time_algorithm_update': 0.0017983698844909668, 'loss': 754.3518713378907, 'time_step': 0.0028801727294921874, 'rollout_return': -32.19619300485} step=1400\n",
      "2023-02-21 00:16.22 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1500.pt\n",
      "2023-02-21 00:16.22 [info     ] DQN_online_20230221001618: epoch=15 step=1500 epoch=15 metrics={'time_inference': 0.00029528379440307617, 'time_environment_step': 0.0005894517898559571, 'time_sample_batch': 7.92527198791504e-05, 'time_algorithm_update': 0.0020247697830200195, 'loss': 632.3071591186523, 'time_step': 0.0030735993385314943, 'rollout_return': -25.275381829999997} step=1500\n",
      "2023-02-21 00:16.22 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1600.pt\n",
      "2023-02-21 00:16.22 [info     ] DQN_online_20230221001618: epoch=16 step=1600 epoch=16 metrics={'time_inference': 0.000269773006439209, 'time_environment_step': 0.0005121016502380371, 'time_sample_batch': 6.818771362304688e-05, 'time_algorithm_update': 0.002329261302947998, 'loss': 550.5070527648926, 'time_step': 0.0032535672187805178, 'rollout_return': -27.606325363000003} step=1600\n",
      "2023-02-21 00:16.22 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1700.pt\n",
      "2023-02-21 00:16.22 [info     ] DQN_online_20230221001618: epoch=17 step=1700 epoch=17 metrics={'time_inference': 0.0002776360511779785, 'time_environment_step': 0.0005340027809143066, 'time_sample_batch': 7.245779037475587e-05, 'time_algorithm_update': 0.002511584758758545, 'loss': 483.13382446289063, 'time_step': 0.0034733700752258303, 'rollout_return': -9.4429186954} step=1700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-21 00:16.23 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1800.pt\n",
      "2023-02-21 00:16.23 [info     ] DQN_online_20230221001618: epoch=18 step=1800 epoch=18 metrics={'time_inference': 0.00032436609268188477, 'time_environment_step': 0.0006135368347167969, 'time_sample_batch': 7.918596267700195e-05, 'time_algorithm_update': 0.003137357234954834, 'loss': 445.033624420166, 'time_step': 0.004249806404113769, 'rollout_return': -38.96307238100124} step=1800\n",
      "2023-02-21 00:16.23 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_1900.pt\n",
      "2023-02-21 00:16.23 [info     ] DQN_online_20230221001618: epoch=19 step=1900 epoch=19 metrics={'time_inference': 0.00028581857681274413, 'time_environment_step': 0.0005219244956970215, 'time_sample_batch': 6.752252578735351e-05, 'time_algorithm_update': 0.003143362998962402, 'loss': 377.0289895629883, 'time_step': 0.004094462394714355, 'rollout_return': 41.992319546800005} step=1900\n",
      "2023-02-21 00:16.24 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2000.pt\n",
      "2023-02-21 00:16.24 [info     ] DQN_online_20230221001618: epoch=20 step=2000 epoch=20 metrics={'time_inference': 0.00028627395629882813, 'time_environment_step': 0.0005383920669555664, 'time_sample_batch': 7.126092910766601e-05, 'time_algorithm_update': 0.0032510781288146973, 'loss': 406.3154432678223, 'time_step': 0.004223437309265137, 'rollout_return': -5.03499036612} step=2000\n",
      "2023-02-21 00:16.24 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2100.pt\n",
      "2023-02-21 00:16.24 [info     ] DQN_online_20230221001618: epoch=21 step=2100 epoch=21 metrics={'time_inference': 0.00033962011337280274, 'time_environment_step': 0.0006507992744445801, 'time_sample_batch': 8.644342422485351e-05, 'time_algorithm_update': 0.0032090115547180175, 'loss': 372.30681838989256, 'time_step': 0.004384198188781738, 'rollout_return': -13.676360997170008} step=2100\n",
      "2023-02-21 00:16.24 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2200.pt\n",
      "2023-02-21 00:16.24 [info     ] DQN_online_20230221001618: epoch=22 step=2200 epoch=22 metrics={'time_inference': 0.0003250908851623535, 'time_environment_step': 0.0006257057189941406, 'time_sample_batch': 8.346319198608398e-05, 'time_algorithm_update': 0.0029118943214416506, 'loss': 411.7319287109375, 'time_step': 0.004039220809936524, 'rollout_return': -26.272983981499994} step=2200\n",
      "2023-02-21 00:16.25 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2300.pt\n",
      "2023-02-21 00:16.25 [info     ] DQN_online_20230221001618: epoch=23 step=2300 epoch=23 metrics={'time_inference': 0.00035305023193359374, 'time_environment_step': 0.0006600117683410644, 'time_sample_batch': 8.73875617980957e-05, 'time_algorithm_update': 0.0030829858779907226, 'loss': 361.2734603881836, 'time_step': 0.004279782772064209, 'rollout_return': -7.353322882267991} step=2300\n",
      "2023-02-21 00:16.25 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2400.pt\n",
      "2023-02-21 00:16.25 [info     ] DQN_online_20230221001618: epoch=24 step=2400 epoch=24 metrics={'time_inference': 0.0003139305114746094, 'time_environment_step': 0.0006127095222473144, 'time_sample_batch': 8.16202163696289e-05, 'time_algorithm_update': 0.002935466766357422, 'loss': 341.83265365600585, 'time_step': 0.004033746719360351, 'rollout_return': 6.970205596035001} step=2400\n",
      "2023-02-21 00:16.26 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2500.pt\n",
      "2023-02-21 00:16.26 [info     ] DQN_online_20230221001618: epoch=25 step=2500 epoch=25 metrics={'time_inference': 0.00031543493270874023, 'time_environment_step': 0.0005907654762268066, 'time_sample_batch': 7.55143165588379e-05, 'time_algorithm_update': 0.0029334163665771483, 'loss': 283.44779647827147, 'time_step': 0.004003713130950928, 'rollout_return': 46.115222298892505} step=2500\n",
      "2023-02-21 00:16.26 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2600.pt\n",
      "2023-02-21 00:16.26 [info     ] DQN_online_20230221001618: epoch=26 step=2600 epoch=26 metrics={'time_inference': 0.0003003597259521484, 'time_environment_step': 0.0005588960647583007, 'time_sample_batch': 7.274627685546875e-05, 'time_algorithm_update': 0.0028946971893310546, 'loss': 299.88093353271483, 'time_step': 0.0039068984985351565, 'rollout_return': -13.267396343639993} step=2600\n",
      "2023-02-21 00:16.26 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2700.pt\n",
      "2023-02-21 00:16.26 [info     ] DQN_online_20230221001618: epoch=27 step=2700 epoch=27 metrics={'time_inference': 0.00031872034072875975, 'time_environment_step': 0.0006113910675048828, 'time_sample_batch': 7.909774780273437e-05, 'time_algorithm_update': 0.00302417516708374, 'loss': 271.06060745239256, 'time_step': 0.004125711917877198, 'rollout_return': 39.78042624757839} step=2700\n",
      "2023-02-21 00:16.27 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2800.pt\n",
      "2023-02-21 00:16.27 [info     ] DQN_online_20230221001618: epoch=28 step=2800 epoch=28 metrics={'time_inference': 0.0002892422676086426, 'time_environment_step': 0.0005412578582763671, 'time_sample_batch': 6.839513778686524e-05, 'time_algorithm_update': 0.002887728214263916, 'loss': 243.5453025817871, 'time_step': 0.0038633322715759277, 'rollout_return': -52.68572107599999} step=2800\n",
      "2023-02-21 00:16.27 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_2900.pt\n",
      "2023-02-21 00:16.27 [info     ] DQN_online_20230221001618: epoch=29 step=2900 epoch=29 metrics={'time_inference': 0.00026373624801635743, 'time_environment_step': 0.0005021214485168457, 'time_sample_batch': 6.541728973388672e-05, 'time_algorithm_update': 0.0028003978729248045, 'loss': 241.7488833618164, 'time_step': 0.0037017250061035156, 'rollout_return': -29.518061871000004} step=2900\n",
      "2023-02-21 00:16.28 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3000.pt\n",
      "2023-02-21 00:16.28 [info     ] DQN_online_20230221001618: epoch=30 step=3000 epoch=30 metrics={'time_inference': 0.0002748870849609375, 'time_environment_step': 0.0005318117141723633, 'time_sample_batch': 6.985664367675781e-05, 'time_algorithm_update': 0.002917311191558838, 'loss': 261.94869720458985, 'time_step': 0.003873124122619629, 'rollout_return': -13.216222873484998} step=3000\n",
      "2023-02-21 00:16.28 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3100.pt\n",
      "2023-02-21 00:16.28 [info     ] DQN_online_20230221001618: epoch=31 step=3100 epoch=31 metrics={'time_inference': 0.00026363611221313476, 'time_environment_step': 0.000503847599029541, 'time_sample_batch': 6.496906280517578e-05, 'time_algorithm_update': 0.0028461313247680666, 'loss': 255.70304229736328, 'time_step': 0.0037482190132141113, 'rollout_return': 72.4419318497125} step=3100\n",
      "2023-02-21 00:16.28 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3200.pt\n",
      "2023-02-21 00:16.28 [info     ] DQN_online_20230221001618: epoch=32 step=3200 epoch=32 metrics={'time_inference': 0.00026888608932495116, 'time_environment_step': 0.0005158162117004394, 'time_sample_batch': 6.762027740478516e-05, 'time_algorithm_update': 0.0029279232025146485, 'loss': 232.32844024658203, 'time_step': 0.003852336406707764, 'rollout_return': -18.859730324550004} step=3200\n",
      "2023-02-21 00:16.29 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3300.pt\n",
      "2023-02-21 00:16.29 [info     ] DQN_online_20230221001618: epoch=33 step=3300 epoch=33 metrics={'time_inference': 0.00031702280044555665, 'time_environment_step': 0.0006046557426452637, 'time_sample_batch': 7.693290710449219e-05, 'time_algorithm_update': 0.0030828356742858888, 'loss': 190.81396377563476, 'time_step': 0.004172415733337403, 'rollout_return': -16.218898491926247} step=3300\n",
      "2023-02-21 00:16.29 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3400.pt\n",
      "2023-02-21 00:16.29 [info     ] DQN_online_20230221001618: epoch=34 step=3400 epoch=34 metrics={'time_inference': 0.0002963137626647949, 'time_environment_step': 0.0005548548698425293, 'time_sample_batch': 7.266044616699219e-05, 'time_algorithm_update': 0.002993927001953125, 'loss': 212.5623122406006, 'time_step': 0.004001140594482422, 'rollout_return': -16.34560286025} step=3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-21 00:16.30 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3500.pt\n",
      "2023-02-21 00:16.30 [info     ] DQN_online_20230221001618: epoch=35 step=3500 epoch=35 metrics={'time_inference': 0.0003439188003540039, 'time_environment_step': 0.0006403923034667969, 'time_sample_batch': 8.721113204956055e-05, 'time_algorithm_update': 0.0031885385513305666, 'loss': 216.09801635742187, 'time_step': 0.004353160858154297, 'rollout_return': -6.818429750088991} step=3500\n",
      "2023-02-21 00:16.30 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3600.pt\n",
      "2023-02-21 00:16.30 [info     ] DQN_online_20230221001618: epoch=36 step=3600 epoch=36 metrics={'time_inference': 0.0002756857872009277, 'time_environment_step': 0.0005082917213439941, 'time_sample_batch': 6.685495376586915e-05, 'time_algorithm_update': 0.002954108715057373, 'loss': 184.0675839996338, 'time_step': 0.0038811230659484865, 'rollout_return': -33.78894783675001} step=3600\n",
      "2023-02-21 00:16.30 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3700.pt\n",
      "2023-02-21 00:16.30 [info     ] DQN_online_20230221001618: epoch=37 step=3700 epoch=37 metrics={'time_inference': 0.0002942967414855957, 'time_environment_step': 0.0005567145347595215, 'time_sample_batch': 7.466077804565429e-05, 'time_algorithm_update': 0.0030748653411865235, 'loss': 179.74370208740234, 'time_step': 0.0040806388854980466, 'rollout_return': 1.3245757304799917} step=3700\n",
      "2023-02-21 00:16.31 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3800.pt\n",
      "2023-02-21 00:16.31 [info     ] DQN_online_20230221001618: epoch=38 step=3800 epoch=38 metrics={'time_inference': 0.00028672933578491214, 'time_environment_step': 0.0005443930625915527, 'time_sample_batch': 7.14111328125e-05, 'time_algorithm_update': 0.0030137419700622557, 'loss': 175.5936309814453, 'time_step': 0.003997180461883545, 'rollout_return': -40.775167980999996} step=3800\n",
      "2023-02-21 00:16.31 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_3900.pt\n",
      "2023-02-21 00:16.31 [info     ] DQN_online_20230221001618: epoch=39 step=3900 epoch=39 metrics={'time_inference': 0.0002762556076049805, 'time_environment_step': 0.0005229783058166504, 'time_sample_batch': 7.065057754516601e-05, 'time_algorithm_update': 0.0028578782081604003, 'loss': 181.3968350982666, 'time_step': 0.0038074851036071777, 'rollout_return': 40.97343541565} step=3900\n",
      "2023-02-21 00:16.32 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4000.pt\n",
      "2023-02-21 00:16.32 [info     ] DQN_online_20230221001618: epoch=40 step=4000 epoch=40 metrics={'time_inference': 0.0002953648567199707, 'time_environment_step': 0.000533452033996582, 'time_sample_batch': 7.2021484375e-05, 'time_algorithm_update': 0.0029569816589355468, 'loss': 160.85313606262207, 'time_step': 0.003933522701263428, 'rollout_return': -36.3489280550125} step=4000\n",
      "2023-02-21 00:16.32 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4100.pt\n",
      "2023-02-21 00:16.32 [info     ] DQN_online_20230221001618: epoch=41 step=4100 epoch=41 metrics={'time_inference': 0.0002807021141052246, 'time_environment_step': 0.0005307483673095703, 'time_sample_batch': 7.134437561035156e-05, 'time_algorithm_update': 0.0028785276412963867, 'loss': 163.20687316894532, 'time_step': 0.0038399600982666014, 'rollout_return': -12.415654240550001} step=4100\n",
      "2023-02-21 00:16.32 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4200.pt\n",
      "2023-02-21 00:16.32 [info     ] DQN_online_20230221001618: epoch=42 step=4200 epoch=42 metrics={'time_inference': 0.0002648472785949707, 'time_environment_step': 0.0004892230033874511, 'time_sample_batch': 6.688833236694336e-05, 'time_algorithm_update': 0.0027880263328552246, 'loss': 156.22279594421386, 'time_step': 0.003680553436279297, 'rollout_return': 80.742567452391} step=4200\n",
      "2023-02-21 00:16.33 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4300.pt\n",
      "2023-02-21 00:16.33 [info     ] DQN_online_20230221001618: epoch=43 step=4300 epoch=43 metrics={'time_inference': 0.0002737569808959961, 'time_environment_step': 0.0005390548706054688, 'time_sample_batch': 6.956815719604493e-05, 'time_algorithm_update': 0.0028426575660705565, 'loss': 155.84935607910157, 'time_step': 0.0037975859642028808, 'rollout_return': -27.313437975009997} step=4300\n",
      "2023-02-21 00:16.33 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4400.pt\n",
      "2023-02-21 00:16.33 [info     ] DQN_online_20230221001618: epoch=44 step=4400 epoch=44 metrics={'time_inference': 0.0002822446823120117, 'time_environment_step': 0.0005479717254638672, 'time_sample_batch': 7.338285446166993e-05, 'time_algorithm_update': 0.0029177165031433108, 'loss': 138.40625686645507, 'time_step': 0.0039002132415771485, 'rollout_return': 3.1537163234289975} step=4400\n",
      "2023-02-21 00:16.34 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4500.pt\n",
      "2023-02-21 00:16.34 [info     ] DQN_online_20230221001618: epoch=45 step=4500 epoch=45 metrics={'time_inference': 0.00026505231857299806, 'time_environment_step': 0.0005054545402526856, 'time_sample_batch': 6.775856018066406e-05, 'time_algorithm_update': 0.0028512001037597655, 'loss': 125.26831665039063, 'time_step': 0.0037626338005065916, 'rollout_return': 78.09289424042} step=4500\n",
      "2023-02-21 00:16.34 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4600.pt\n",
      "2023-02-21 00:16.34 [info     ] DQN_online_20230221001618: epoch=46 step=4600 epoch=46 metrics={'time_inference': 0.0003039860725402832, 'time_environment_step': 0.0005730128288269043, 'time_sample_batch': 7.982730865478515e-05, 'time_algorithm_update': 0.003007001876831055, 'loss': 123.83777530670166, 'time_step': 0.0040476608276367185, 'rollout_return': 19.76993158602498} step=4600\n",
      "2023-02-21 00:16.34 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4700.pt\n",
      "2023-02-21 00:16.34 [info     ] DQN_online_20230221001618: epoch=47 step=4700 epoch=47 metrics={'time_inference': 0.0003782534599304199, 'time_environment_step': 0.0007088232040405273, 'time_sample_batch': 9.511709213256836e-05, 'time_algorithm_update': 0.0032496905326843263, 'loss': 130.23350860595704, 'time_step': 0.004535324573516845, 'rollout_return': 11.246206795254993} step=4700\n",
      "2023-02-21 00:16.35 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4800.pt\n",
      "2023-02-21 00:16.35 [info     ] DQN_online_20230221001618: epoch=48 step=4800 epoch=48 metrics={'time_inference': 0.00030602455139160155, 'time_environment_step': 0.0005598187446594239, 'time_sample_batch': 7.673740386962891e-05, 'time_algorithm_update': 0.0030368757247924805, 'loss': 112.61642288208007, 'time_step': 0.004066438674926758, 'rollout_return': -6.138632905041998} step=4800\n",
      "2023-02-21 00:16.35 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_4900.pt\n",
      "2023-02-21 00:16.35 [info     ] DQN_online_20230221001618: epoch=49 step=4900 epoch=49 metrics={'time_inference': 0.00025681257247924807, 'time_environment_step': 0.0004898452758789062, 'time_sample_batch': 6.561517715454101e-05, 'time_algorithm_update': 0.0028771877288818358, 'loss': 120.44222663879394, 'time_step': 0.0037631464004516603, 'rollout_return': 8.466188175820003} step=4900\n",
      "2023-02-21 00:16.36 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_5000.pt\n",
      "2023-02-21 00:16.36 [info     ] DQN_online_20230221001618: epoch=50 step=5000 epoch=50 metrics={'time_inference': 0.00040342330932617186, 'time_environment_step': 0.0007474350929260253, 'time_sample_batch': 0.00010318279266357421, 'time_algorithm_update': 0.0034917330741882323, 'loss': 113.25048389434815, 'time_step': 0.004862148761749268, 'rollout_return': 120.70402694399998} step=5000\n",
      "2023-02-21 00:16.36 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_5100.pt\n",
      "2023-02-21 00:16.36 [info     ] DQN_online_20230221001618: epoch=51 step=5100 epoch=51 metrics={'time_inference': 0.0002897191047668457, 'time_environment_step': 0.0005540752410888672, 'time_sample_batch': 7.509469985961915e-05, 'time_algorithm_update': 0.003059666156768799, 'loss': 100.08088459014893, 'time_step': 0.004056379795074463, 'rollout_return': -31.649461343500008} step=5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-21 00:16.37 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_5200.pt\n",
      "2023-02-21 00:16.37 [info     ] DQN_online_20230221001618: epoch=52 step=5200 epoch=52 metrics={'time_inference': 0.0002756214141845703, 'time_environment_step': 0.0005066537857055664, 'time_sample_batch': 6.560802459716797e-05, 'time_algorithm_update': 0.00290114164352417, 'loss': 95.48711597442627, 'time_step': 0.0038222122192382814, 'rollout_return': -39.832320737675005} step=5200\n",
      "2023-02-21 00:16.37 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_5300.pt\n",
      "2023-02-21 00:16.37 [info     ] DQN_online_20230221001618: epoch=53 step=5300 epoch=53 metrics={'time_inference': 0.00029144525527954104, 'time_environment_step': 0.0005518889427185059, 'time_sample_batch': 7.634401321411132e-05, 'time_algorithm_update': 0.002886917591094971, 'loss': 104.77423648834228, 'time_step': 0.0038869643211364747, 'rollout_return': -47.159345641099996} step=5300\n",
      "2023-02-21 00:16.37 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20230221001618/model_5400.pt\n",
      "2023-02-21 00:16.37 [info     ] DQN_online_20230221001618: epoch=54 step=5400 epoch=54 metrics={'time_inference': 0.00027343273162841794, 'time_environment_step': 0.0005298805236816406, 'time_sample_batch': 7.181882858276368e-05, 'time_algorithm_update': 0.00265117883682251, 'loss': 92.95440078735352, 'time_step': 0.0036004877090454104, 'rollout_return': 10.036357479588002} step=5400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# | include: false\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_online\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# train for 100K steps\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# evaluation is performed every 100 steps\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_start_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# parameter update starts after 100 steps\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# tensorboard_dir= 'runs'\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/algos/base.py:251\u001b[0m, in \u001b[0;36mAlgoBase.fit_online\u001b[0;34m(self, env, buffer, explorer, n_steps, n_steps_per_epoch, update_interval, update_start_step, random_steps, eval_env, eval_epsilon, save_metrics, save_interval, experiment_name, with_timestamp, logdir, verbose, show_progress, tensorboard_dir, timelimit_aware, callback)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# check action-space\u001b[39;00m\n\u001b[1;32m    249\u001b[0m _assert_action_space(\u001b[38;5;28mself\u001b[39m, env)\n\u001b[0;32m--> 251\u001b[0m \u001b[43mtrain_single_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_start_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate_start_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_timestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_timestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimelimit_aware\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimelimit_aware\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/online/iterators.py:211\u001b[0m, in \u001b[0;36mtrain_single_env\u001b[0;34m(algo, env, buffer, explorer, n_steps, n_steps_per_epoch, update_interval, update_start_step, random_steps, eval_env, eval_epsilon, save_metrics, save_interval, experiment_name, with_timestamp, logdir, verbose, show_progress, tensorboard_dir, timelimit_aware, callback)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m explorer:\n\u001b[1;32m    210\u001b[0m     x \u001b[38;5;241m=\u001b[39m fed_observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m fed_observation\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 211\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mexplorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_step\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     action \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39msample_action([fed_observation])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/online/explorers.py:94\u001b[0m, in \u001b[0;36mLinearDecayEpsilonGreedy.sample\u001b[0;34m(self, algo, x, step)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m, algo: _ActionProtocol, x: np\u001b[38;5;241m.\u001b[39mndarray, step: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     82\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns :math:`\\\\epsilon`-greedy action.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     greedy_actions \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     random_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(algo\u001b[38;5;241m.\u001b[39maction_size, size\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     96\u001b[0m     is_random \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_epsilon(step)\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/algos/base.py:127\u001b[0m, in \u001b[0;36mAlgoBase.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"Returns greedy actions.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m.. code-block:: python\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, IMPL_NOT_INITIALIZED_ERROR\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_best_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/torch_utility.py:304\u001b[0m, in \u001b[0;36meval_api.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mset_eval_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/d3rlpy/torch_utility.py:57\u001b[0m, in \u001b[0;36mset_eval_mode\u001b[0;34m(impl)\u001b[0m\n\u001b[1;32m     55\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(impl, key)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/nn/modules/module.py:1858\u001b[0m, in \u001b[0;36mModule.eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1843\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the module in evaluation mode.\u001b[39;00m\n\u001b[1;32m   1844\u001b[0m \n\u001b[1;32m   1845\u001b[0m \u001b[38;5;124;03m    This has any effect only on certain modules. See documentations of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/nn/modules/module.py:1839\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 1839\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/nn/modules/module.py:1837\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1837\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   1839\u001b[0m     module\u001b[38;5;241m.\u001b[39mtrain(mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/nn/modules/module.py:1220\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1217\u001b[0m                 d\u001b[38;5;241m.\u001b[39mdiscard(name)\n\u001b[1;32m   1219\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParameter\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign parameters before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/nn/parameter.py:10\u001b[0m, in \u001b[0;36m_ParameterMeta.__instancecheck__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__instancecheck__\u001b[39;49m(instance) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(instance, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(instance, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_param\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# | include: false\n",
    "\n",
    "dqn.fit_online(\n",
    "    env,\n",
    "    buffer,\n",
    "    explorer,\n",
    "    n_steps=100000,  # train for 100K steps\n",
    "    n_steps_per_epoch=100,  # evaluation is performed every 100 steps\n",
    "    update_start_step=100,  # parameter update starts after 100 steps\n",
    "    eval_epsilon=0.3,\n",
    "    save_metrics=True,\n",
    "    # tensorboard_dir= 'runs'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
