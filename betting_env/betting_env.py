# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_betting_env.ipynb.

# %% auto 0
__all__ = ['module_path', 'Observation', 'BettingEnv', 'uncache', 'torch_api', 'CustomScaler']

# %% ../nbs/00_betting_env.ipynb 3
import pandas as pd
import warnings
import gym
import numpy as np
import numexpr
import json
import os
import sys
import torch

module_path = os.path.abspath(os.path.join(".."))
if module_path not in sys.path:
    sys.path.append(module_path)
from .asian_handicap_pnl import *
from .datastructure.odds import MarketOdds
from .config.mongo import mongo_init
from pymatchpred.datastructure.lineup import TeamSheet
from pymatchpred.datastructure.features import GameFeatures
from d3rlpy.preprocessing import ActionScaler, RewardScaler, Scaler
from d3rlpy.dataset import TransitionMiniBatch
from inspect import signature
from typing_extensions import Protocol
from infi.traceback import pretty_traceback_and_exit_decorator
from pandas.core.common import SettingWithCopyWarning
from typing import (
    Any,
    Callable,
    ClassVar,
    Dict,
    List,
    Optional,
    Sequence,
    Union,
    Type,
    Tuple,
)

warnings.simplefilter(action="ignore", category=SettingWithCopyWarning)

# %% ../nbs/00_betting_env.ipynb 6
class Observation:
    def __init__(
        self,
        game_id: int,  # game Id
        lineups: np.ndarray,  # lineups
        lineups_ids: np.ndarray,  # lineups opta Ids
        teams_names: pd.core.series.Series,  # team names
        teams_ids: np.ndarray,  # teams opta Ids
        betting_market: np.ndarray,  # odds
        ah_line: float,  # Asian handicap line
    ):
        self.game_id = game_id
        self.lineups = lineups
        self.lineups_ids = lineups_ids
        self.teams_names = teams_names
        self.teams_ids = teams_ids
        self.betting_market = betting_market
        self.ah_line = ah_line

    def __call__(self):
        """numerical output"""
        numerical_observation = np.concatenate(
            (
                np.array([self.game_id]).reshape(1, -1),  # opta gameId
                np.array([self.teams_ids]),  # teams Opta Ids
                np.array([self.lineups_ids[0]]),  # home lineup (players opta Id)
                np.array([self.lineups_ids[1]]),  # away lineup (players opta Id)
                self.betting_market,  # odds (1x2 and AH)
            ),
            axis=1,
        )

        return numerical_observation

    def observation_pretty_output(self):
        """User-friendly output"""
        observation = {
            "gameId": [self.game_id],
            "homeTeam": [self.teams_names[0]],
            "awayTeam": [self.teams_names[1]],
            "homeLineup": self.lineups[0],
            "awayLineup": self.lineups[1],
            "odds1": self.betting_market[:, 0:3][0][0],
            "oddsX": self.betting_market[:, 0:3][0][1],
            "odds2": self.betting_market[:, 0:3][0][2],
            "ahLine": [self.ah_line],
            "oddsAhHome": self.betting_market[:, 3:][0][0],
            "oddsAhAway": self.betting_market[:, 3:][0][1],
        }

        return pd.DataFrame(observation, index=[0])

# %% ../nbs/00_betting_env.ipynb 9
class BettingEnv(gym.Env):
    """Base class for sports betting environments.

    Creates an OpenAI Gym environment that supports betting a (small / medium / large) amount
    on a single outcome for a single game.

    Parameters
    ----------
    observation_space : gym.spaces.Box
        The observation space for the environment.
        The observation space shape is (1, N) where N is the number of possible
        outcomes for the game + len(gameId, 2 lineups, ah line) .

    action_space : gym.spaces.Discrete
        The action space for the environment.
        The action space is a set of choices that the agent can do.

    balance : float
        The current balance of the environment.

    starting_bank : int, default=100
        The starting bank / balance for the environment.
    """

    metadata = {"render_modes": ["human"]}
    
    # actions
    ACTIONS_LIST = [
        [0, 0, 0, 0, 0],  # no bets
        [0.05, 0, 0, 0, 0],  # betting on home team (1x2)
        [0.4, 0, 0, 0, 0],  # betting on home team (1x2)
        [0.7, 0, 0, 0, 0],  # betting on home team (1x2)
        [0, 0, 0.05, 0, 0],  # betting on away team (1x2)
        [0, 0, 0.4, 0, 0],  # betting on away team (1x2)
        [0, 0, 0.7, 0, 0],  # betting on away team (1x2)
        [0, 0.05, 0, 0, 0],  # betting on draw (1x2)
        [0, 0.4, 0, 0, 0],  # betting on draw (1x2)
        [0, 0.7, 0, 0, 0],  # betting on draw (1x2)
        [0, 0, 0, 0.05, 0],  # betting on home (Asian Handicap)
        [0, 0, 0, 0.4, 0],  # betting on home (Asian Handicap)
        [0, 0, 0, 0.7, 0],  # betting on home (Asian Handicap)
        [0, 0, 0, 0, 0.05],  # betting on away (Asian Handicap)
        [0, 0, 0, 0, 0.4],  # betting on away (Asian Handicap)
        [0, 0, 0, 0, 0.7],  # betting on away (Asian Handicap)
    ]

    def __init__(
        self,
        game_odds,
        odds_column_names=[
            "preGameOdds1",
            "preGameOdds2",
            "preGameOddsX",
            "preGameAhHome",
            "preGameAhAway",
        ],
        starting_bank=100,
    ):
        """Initializes a new environment

        Parameters
        ----------
        game_odds: pandas dataframe
            A list of games, with their betting odds.
        odds_column_names: list of str
            A list of column names with length == number of odds.
        bet_size: list
            3 possible bets : small, medium and large
        starting_bank: int
            bank account

        """

        super().__init__()
        # games df
        self._game = game_odds.copy()
        # sort data by date
        if "gameDate" in self._game.columns:
            self._game["gameDate"] = pd.to_datetime(self._game["gameDate"])
            self._game = self._game.sort_values(by="gameDate")
        # odds (1X2 and Asian handicap) values
        self._odds = self._game[odds_column_names].values
        # results
        self._results = self._game["result"].values
        # ah lines
        self._lines = self._game["lineId"].values
        # game goal-difference
        self._gd = self._game["postGameGd"].values
        # teams names
        self._teams_names = self._game[["homeTeamName", "awayTeamName"]]
        # teams opta id
        self._teams_ids = self._game[["homeTeamOptaId", "awayTeamOptaId"]].values

        # teams lineups (names and positions)
        self._lineups = self._game[["homeTeamLineup", "awayTeamLineup"]].values
        # teams lineups (opta ids)
        self._lineups_ids = self._game[
            ["homeTeamLineupIds", "awayTeamLineupIds"]
        ].values
        # games ids
        self._game_ids = self._game["optaGameId"].values
        # observation space
        self.observation_space = gym.spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(
                self._odds.shape[1] + 25,
            ),  # 25 = 22(players Ids) + 2(home and away team ids) + 1(gameId)
            dtype=np.float64,
        )
        # actions space
        self.action_space = gym.spaces.Discrete(
            len(BettingEnv.ACTIONS_LIST)
        )  # betting action
        # env balance
        self.balance = self.starting_bank = starting_bank
        # current step (game)
        self.current_step = 0
        # bet size for each outcome
        self.bet_size_matrix = None

    def _get_current_index(self):
        return self.current_step % self._odds.shape[0]

    def get_odds(self):
        """Returns the odds for the current step.

        Returns
        -------
        odds : numpy.ndarray of shape (1, n_odds)
            The odds for the current step.
        """
        return pd.DataFrame([self._odds[self.current_step]]).values

    def get_bet(self, action):
        """Returns the betting matrix for the action provided.

        Parameters
        ----------
        action : int
            An action provided by the agent.

        Returns
        -------
        bet : array of shape (1, n_odds)
            The betting matrix, where each outcome specified in the action
            has a value of 1 and 0 otherwise.
        """
        return BettingEnv.ACTIONS_LIST[action]

    @pretty_traceback_and_exit_decorator
    def step(self, action):
        """Run one timestep of the environment's dynamics. When end of episode is reached,
        you are responsible for calling reset() to reset this environment's state.

        Accepts an action and returns a tuple (observation, reward, done, info).

        Parameters
        ----------
        action : int
            An action provided by the agent.

        Returns
        -------
        observation : dataframe
            The agent's observation of the current environment
        reward : float
            The amount of reward returned after previous action
        done : bool
            Whether the episode has ended, in which case further step() calls will return undefined results
        info : dict
            Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)

        """
        # init observation
        observation = np.ones(shape=self.observation_space.shape)
        # reward
        reward = 0
        # finish
        done = False
        # episode info
        info = self.create_info(action)
        
        if self.balance < 1:  # no more money
            done = True
        else:
            # bet action
            bet = self.get_bet(action)
            # game result
            results = self.get_results()
            if self.legal_bet(bet):  # making sure agent has enough money for the bet
                # current odds
                odds = self.get_odds()
                # reward (positive or negative)
                reward = self.get_reward(bet, odds, results)
                # update balance
                self.balance += reward
                info.update(legal_bet=True)
            else:
                reward = -(bet * self.bet_size_matrix).sum()
            # update info
            info.update(results=results.argmax())
            info.update(reward=reward)
            # increment step
            self.current_step += 1
            # check if we are finished
            if self.finish():
                done = True
            else:
                observation = self.get_observation()
        
        # update flag
        info.update(done=done)
        # return
        return observation, reward, done, info

    def get_observation(self):
        """return the observation of the current step.

        Returns
        -------
        obs : numpy.ndarray of shape (1, n_odds + 22)
            The observation of the current step.
        """
        # current game
        index = self._get_current_index()
        # current game id
        game_id = self._game_ids[index]
        # current game lineups
        lineups = self._lineups[index]
        lineups_ids = self._lineups_ids[index]
        # teams
        teams_names = self._teams_names.iloc[index]
        teams_ids = self._teams_ids[index]
        # 1X2 and AH odds
        betting_market = self.get_odds()
        # chosen line (AH line)
        ah_line = self._lines[index]

        # observation
        observation = Observation(
            game_id,
            lineups,
            lineups_ids,
            teams_names,
            teams_ids,
            betting_market,
            ah_line,
        )
        observation = observation().reshape(self.observation_space.shape)

        return observation

    def get_reward(self, bet, odds, results):
        """Calculates the reward

        Parameters
        ----------
        bet : array of shape (1, n_odds)
        odds: dataframe of shape (1, n_odds)
            A games with its betting odds.
        results : array of shape (1, n_odds)

        Returns
        -------
        reward : float
            The amount of reward returned after previous action
        """
        # agent choice
        bet_index = np.argmax(np.array(bet))
        # bet size
        bet_size_matrix = self.bet_size_matrix
        # balance
        balance = self.balance
        # if the action is a AH bet
        if bet_index in [3, 4]:
            # game goal_difference
            obs_gd = (
                self._gd[self.current_step]
                if bet_index == 3
                else -self._gd[self.current_step]
            )
            # ah line
            ah_line = float(
                self._lines[self.current_step]
                if bet_index == 3
                else -self._lines[self.current_step]
            )
            # ah side odds
            ah_odds = (
                odds[:, 3:4][0].item() if bet_index == 3 else odds[:, 4:][0].item()
            )
            # calculate profit
            profit = AsianHandicap.pnl(obs_gd, ah_line, ah_odds)
            profit = (
                0 if profit is None else numexpr.evaluate("sum(bet * balance * profit)")
            )
        else:  # case 1X2
            reward = numexpr.evaluate("sum(bet * balance * results * odds)")
            expense = numexpr.evaluate("sum(bet * balance)")
            profit = reward - expense

        return profit

    def reset(self):
        """Resets the state of the environment and returns an initial observation.

        Returns
        -------
        observation : dataframe
            the initial observation.
        """
        self.balance = self.starting_bank
        self.current_step = 0
        return self.get_observation()

    def render(self, mode="human"):
        """Outputs the current balance and the current step.

        Returns
        -------
        msg : str
            A string with the current balance,
            the current step and the current game info.
        """
        index = self._get_current_index()
        teams = self._teams_names.iloc[index]
        game_id = self._game_ids[index]
        teams = teams.itertuples() if isinstance(teams, pd.DataFrame) else [teams]
        teams_str = ", ".join(
            [
                "Home Team: {} VS Away Team: {}".format(
                    row.homeTeamName, row.awayTeamName
                )
                for row in teams
            ]
        )

        print("Current balance at step {}: {}".format(self.current_step, self.balance))
        print("Current game id : {}".format(game_id))
        print(teams_str)

    def finish(self):
        """Checks if the episode has reached an end.

        The episode has reached an end if there are no more games to bet.

        Returns
        -------
        finish : bool
            True if the current_step is equal to n_games, False otherwise
        """
        return self.current_step == self._odds.shape[0]  # no more games left to bet

    def get_results(self):
        """Returns the results matrix for the current step.

        Returns
        -------
        result : array of shape (1, n_odds)
            The result matrix, where the index of the outcome that happened
            value is 1 and the rest of the indexes values are 0.
        """
        result = np.zeros(shape=(1, self._odds.shape[1]))
        result[
            np.arange(result.shape[0], dtype=np.int32),
            np.array([self._results[self.current_step]], dtype=np.int32),
        ] = 1

        return result

    def legal_bet(self, bet):
        """Checks if the bet is legal.

        Checks that the bet does not exceed the current balance.

        Parameters
        ----------
        bet : array of shape (1, n_odds)
            The bet to check.

        Returns
        -------
        legal : bool
            True if the bet is legal, False otherwise.
        """
        bet_size = sum([b * self.balance for b in bet])
        return bet_size <= self.balance

    def create_info(self, action):
        """Creates the info dictionary for the given action.

        The info dictionary holds the following information:
            * the current step
            * game odds of the current step
            * bet action of the current step
            * bet size of the current step
            * the balance at the start of the current step
            * reward of the current step
            * game result of the current step
            * state of the current step
        Parameters
        ----------
        action : int
            An action provided by the agent.

        Returns
        -------
        info : dict
            The info dictionary.
        """
        return {
            "current_step": self.current_step,
            "odds": self.get_odds(),
            "bet_action": env.ACTIONS_LIST[action],
            "balance": self.balance,
            "reward": 0,
            "legal_bet": False,
            "results": None,
            "done": False,
        }

# %% ../nbs/00_betting_env.ipynb 21
from d3rlpy import torch_utility
from d3rlpy.torch_utility import _WithDeviceAndScalerProtocol, TorchMiniBatch

# %% ../nbs/00_betting_env.ipynb 23
def uncache(exclude):
    """
    Remove package modules from cache except excluded ones.
    On next import they will be reloaded.
    
    Args:
        exclude (iter<str>): Sequence of module paths.
    """
    
    pkgs = []
    for mod in exclude:
        pkg = mod.split('.', 1)[0]
        pkgs.append(pkg)

    to_uncache = []
    for mod in sys.modules:
        if mod in exclude:
            continue

        if mod in pkgs:
            to_uncache.append(mod)
            continue

        for pkg in pkgs:
            if mod.startswith(pkg + '.'):
                to_uncache.append(mod)
                break

    for mod in to_uncache:
        del sys.modules[mod]


# %% ../nbs/00_betting_env.ipynb 25
def torch_api(
    scaler_targets: Optional[List[str]] = None,
    action_scaler_targets: Optional[List[str]] = None,
    reward_scaler_targets: Optional[List[str]] = None,
) -> Callable[..., np.ndarray]:
    def _torch_api(f: Callable[..., np.ndarray]) -> Callable[..., np.ndarray]:
        # get argument names
        sig = signature(f)
        arg_keys = list(sig.parameters.keys())[1:]
        
        def wrapper(
            self: _WithDeviceAndScalerProtocol, *args: Any, **kwargs: Any
        ) -> np.ndarray:
            tensors: List[Union[torch.Tensor, TorchMiniBatch]] = []
            # convert all args to torch.Tensor
            for i, val in enumerate(args):
                tensor: Union[torch.Tensor, TorchMiniBatch]
                if isinstance(val, torch.Tensor):
                    tensor = val
                elif isinstance(val, list):
                    tensor = default_collate(val)
                    tensor = tensor.to(self.device)
                elif isinstance(val, np.ndarray) or isinstance(val, Observation):
                    if val.dtype == np.uint8:
                        dtype = torch.uint8
                    else:
                        dtype = torch.float32
                    tensor = torch.tensor(
                        data=val,
                        dtype=dtype,
                        device=self.device,
                    )
                elif val is None:
                    tensor = None
                elif isinstance(val, TransitionMiniBatch):
                    tensor = TorchMiniBatch(
                        val,
                        self.device,
                        scaler=self.scaler,
                        action_scaler=self.action_scaler,
                        reward_scaler=self.reward_scaler,
                    )
                else:
                    tensor = torch.tensor(
                        data=val,
                        dtype=torch.float32,
                        device=self.device,
                    )

                if isinstance(tensor, torch.Tensor) or isinstance(val, Observation):
                    # preprocess
                    if self.scaler and scaler_targets:
                        if arg_keys[i] in scaler_targets:
                            tensor = self.scaler.transform(tensor)

                    # preprocess action
                    if self.action_scaler and action_scaler_targets:
                        if arg_keys[i] in action_scaler_targets:
                            tensor = self.action_scaler.transform(tensor)

                    # preprocessing reward
                    if self.reward_scaler and reward_scaler_targets:
                        if arg_keys[i] in reward_scaler_targets:
                            tensor = self.reward_scaler.transform(tensor)

                    # make sure if the tensor is float32 type
                    if tensor is not None and tensor.dtype != torch.float32:
                        tensor = tensor.float()

                tensors.append(tensor)
            return f(self, *tensors, **kwargs)

        return wrapper

    return _torch_api


torch_utility.torch_api = torch_api
uncache(["d3rlpy.torch_utility"])

# %% ../nbs/00_betting_env.ipynb 27
from d3rlpy.algos import DQN
from torch.optim import Adam
from d3rlpy.online.explorers import LinearDecayEpsilonGreedy
from d3rlpy.online.buffers import ReplayBuffer
from d3rlpy.models.optimizers import OptimizerFactory
from d3rlpy.preprocessing.scalers import Scaler, register_scaler

# %% ../nbs/00_betting_env.ipynb 29
class CustomScaler(Scaler):
    def __init__(self):
        pass

    def fit_with_env(self, env: gym.Env):
        pass

    def transform(self, x: torch.Tensor) -> torch.Tensor:
        # add numerical data to observations
        observations = x
        return observations

    def reverse_transform(self, x: torch.Tensor) -> torch.Tensor:
        pass

    def get_params(self, deep: bool = False) -> Dict[str, Any]:
        return {}
